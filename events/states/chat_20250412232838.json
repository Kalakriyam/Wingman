[
  {
    "role": "user",
    "content": "Ik wil het over deze tekst hebben met je.\n\n(Pasted clipboard content:)\nOkay, so in this video I want to talk about using the Gemini models,\nin particular, the new Gemini 2.5, for doing things with audio.\nSo basically getting transcripts out, getting Diarization,\nbeing able to do question and answering over audio, et cetera.\nGemini 2.5 Pro Experimental Blog\nAnd this is a technique that I'm finding myself using more and more,\nwhether I want to summarize a podcast that I don't have time to listen to,\nright through to automated sort of questioning over audio at\nscale to be able to find quotes, and key information, et cetera.\nSo how does Gemini 2.5 change the game in this way?\nAt the start, it really wasn't something that Google or\nDeepMind really posted about.\nIf you come through and look at the, initial blog post here, they\nbarely mentioned, audio at all.\nThey just mentioned that this is, a high quality multimodal model.\nAnd we know that the Gemini models have been multimodal from the start.\nand certainly last year, we were able to basically pass audio into\nthese things with no problem.\nGemini 2.5 Pro Capabilities\nThe big game changer here is that the Gemini 2.5 Pro model, actually\ncan generate 64,000 tokens out.\nAnd that's compared to the earlier Gemini models, which only were\ngenerating say, 8,000 tokens out.\nNow, what does that mean and why is that important?\nBasically, it works out that if you wanna do transcription with one of these\nmodels for about 15 minutes of audio, you are looking at roughly 8,000 tokens.\nOutput Tokens\nso while the Gemini 1.5 Pro models could do some of these tasks and\nthe 2.5 is definitely better though, the challenge was not actually the\nquality of it analyzing, but just being able to get it to generate out the\nnumber of tokens needed to do a full transcript of a podcast , et cetera.\nNow with 64,000 tokens, we can generate out roughly two hours,\nof audio transcripts from this.\nAnd I'll talk about some tricks that you can do with even longer, audio when\nwe get to the code version, So just this past week, Google has basically\nPricing\nannounced the pricing for the 2.50 Pro, and it turns out that it's actually\nvery reasonable, when we are looking at, the different sizes, et cetera.\nSo until we see a flash 2.50, which perhaps will have a much longer generation\nability out, this model is certainly the model that I've been using for\naudio transcripts and audio analysis.\nI'm gonna show you a little bit of my pipeline and what I've been playing\naround with it when we look at it in code.\nSupported Audio Formats\nAlright, so if we come in and look at the docs, we can see that we can actually\nuse a number of different, audio types.\nSo mostly I've been using web files, and mp3s.\nYou can certain use the AAC format or even FLAC files, et cetera.\none of the key things to understand about this, is actually when we\nTechnical Details About Audio\nlook at the technical details here.\nBasically for each second of audio, that's gonna be 32 tokens, in this Gemini model.\nWhich translates to 1,920 tokens per minute or roughly about\n115,000 tokens per hour of audio.\nNow that can be something that you want to think about when you're\nlooking at the pricing being different.\nif it's going to be over, 200,000 tokens, you may want to build a pipeline that\nactually splits so you're never going over that 200,000 token limit where you've\ngot basically, 115,000 going in and say 30 to 40,000 tokens coming out, But if\nyou want to, you can put in a lot more.\nNow, obviously they talk about here that a single prompt can actually\nhave 9.5 hours, of audio in there.\nLike I said, that is fine if you are basically just looking to do\nsome kind of analysis on the audio.\nif you want to do transcripts though, you're gonna find you're probably\nlimited to around two hours, which there's gonna be 230,000 tokens\nin and roughly 64,000 tokens out.\nNow it's important that you also understand that they basically are\ndown sampling the audio to 16K.\npersonally, I don't see that as being a big issue.\nwhat may be an issue for some people is that it takes stereo sources and\nbounces them down to a single channel.\nSo if you did have something that you're trying to basically ask the\nmodel about, positioning in stereo or something that's not gonna work in here.\nNow, one of the key things to basically get this going is you\nwant to upload the audio file.\nand you've got two ways of doing that.\nYou can actually just put that in line with the prompt.\nThat probably doesn't make a lot of sense\nbecause Gemini is going to limit you to, each single call being a maximum\nof 20 mb. Whereas if you use the upload API, you can upload, a single\nfile of 2GB, and be able to use that.\nAnd you can upload multiple files, and then use them in one single call as well.\nSo we'll look at how to do this in the CoLab walkthrough in a bit.\nBut you can see once you've got this uploaded, you can basically\nthen just pass that in as part of the contents, that you're gonna\nuse to generate, new content out.\nand the model is very good as we'll see at being able to generate\ntimestamps out, as you go through this.\nAnd you can see one of the tricks that, I was saying is that if you\ndid wanna do something that's longer than, two hours, you can upload the\naudio file no problem, and just tell it that, okay, I want the transcript\nto start at, x minutes and seconds in, and go to X minutes and seconds out.\nso the model turns out to be quite good at being able to do that as well.\nAlright, let's jump into the code and see how we can actually do all of this.\nDemo (Colab)\nOkay, so looking at the code, I've basically taken some of the examples\nfrom the Gemini team And modified it to exactly what I wanted to have, in here.\nso at the start we're basically just getting the key, et\ncetera, and setting up a client.\nand then this is the original prompt that I took from one\nof the Gemini dev rail people.\nyou can play around with this prompt.\nI've started to sort of make my own custom versions, but I've\njust put their one in here so that you can try this out yourself.\nI really feel with all these things you kind of want to think about\nhow your use case may be slightly different, et and then try and\nincorporate that into the prompt.\nI'm still a big believer of sort of coming up with, a generic prompt or\ntaking a generic prompt and then putting it back into a language model to get\nthe language model to enhance it.\nSo I usually do that with Claude.\nI've done a whole video about meta prompt with Claude, et cetera.\nThe same thing can be done with the Gemini models as well.\nso one of the nice things in here is that it's basically asking it to make a\ntranscript where you're going to have, sort of, one sentence, per line in here.\nNow the cool thing is that it can kind of work out, who the people are.\nNow you can actually put names, in here.\nand it will then, use those names.\nfor the podcast that I put in here, it was able to work out the names.\nAudio Diarization Process\nNow, I think that happens a lot because the model's actually able to\ndo not only, sort of work out what the words are saying, it's actually\ndoing things like audio diarization.\nSo what is Audio diarization?\nAudio diarization, is able to work out which speaker is saying what.\nNow, while it doesn't know, the exact name of who is saying what and who is\nspeaking first, et cetera, it turns out in podcasts when it's two people\nor a group of people, they tend to address each other by name.\nSo they might say, Sam, what do you think?\nAnd then the model's actually smart enough to work out then that the\nnext person talking is probably Sam.\nand if it gets this over a number of times, it can work it out very well.\nSo traditionally the audio diarization was done through sort of extracting,\nalmost like embeddings of each speaker and then doing a sort of clustering or\nsegmentation, et cetera, to work out like which speaker is speaking when.\nThe Gemini model seems to just do this out of the box.\nAnd that's pretty amazing that it, can do that.\nNow if the speakers are not mentioned, you can actually put in the speaker\nnames here as a list of speakers.\nand it will then try to work out, who is who, as it goes through.\nThis is especially useful if you've got speakers, with names where their spelling\nis, slightly different, et cetera.\nAll right.\nSo finding the actual podcast and finding somewhere we can actually\ndownload the MP3, is also a challenge.\nI've looked at a number of different sites.\nAnd some of them you can sort of automate so that you can sort of scrape where the\nlink to the MP3 is and then download it.\nfor this video I've sort of used this site called Pod Bay fm. and what this\nallows you to do is basically just come in here and select, download file and\nthen you can download an MP3 of it.\nSo you can see I've basically downloaded the MP3.\nI've uploaded it into Google CoLab.\nthis particular podcast is from the My First Million, podcast.\nand so it's got two people, speaking.\nYou can see I've just uploaded the MP3 there.\nand if we now, run this, we can see that sure enough it gets uploaded.\nwe've got a name for the file.\nwe've got some information about what it is, when it was created, et cetera.\nAnd it's most important that we keep this uploaded file,\nreference 'cause that's what we're gonna pass into the actual call.\nSo you can see now once we've done with the uploading, doing the\nactual call is actually very simple.\nWe just basically pass it into generate content, pick out the model.\nIn this case, I'm using Gemini 2.5 Pro, and can see the contents is\nbasically the prompt, that we've formatted, above and the uploaded file.\nAnd you'll see that Sure enough, we get back a transcript here.\nWe've got no sort of prefix or anything.\nwe're just getting the raw transcript.\nNow, that is good in some ways and not so good in other ways.\nit's good in that, yeah, okay, we're getting like one line almost\nsort of like a timestamp for each sentence as we go through here.\nThat's probably not that useful when we've got sort of like these short\nsentences, and each of them is taking up a whole line and a timestamp.\nSo what I did was basically put together some code.\nIn fact, actually just Gemini 2.5 wrote this for me, with me prompting it, to\nbasically go through and look for, and it's actually kind of nice because as\nit had some errors, it basically put the debugging, code in there and I was\nable to just un comment this, give it the print statements out, it could see\nwhat was not working, and then be able to fix that code up, pretty quickly.\nSo the idea here was that we're gonna pass in that original text, and\nwe're gonna sort of say like, okay, if it's the same speaker speaking,\nthen only give me a timestamp every 30 seconds or give me a timestamp\nwhen the person speaking changes.\nSo you can see when we've got this process transcript.\nnow I've got, a long bit of text, with the speaker.\nand I only get a new timestamp after 30 seconds.\nBut if someone new comes in, even if they're just saying one word like this,\nokay, we're getting a new timestamp.\nand then we back to the sort of another timestamp for the new\nspeaker and then every 30 seconds.\nSo this simplifies it down, a lot.\nSo that you've got something which still has got all the diarization\ndetail of knowing who is speaking when, but is not just doing a\ntimestamp for every single sentence.\nnow you can obviously change that.\nSo I've put it for every 30 seconds.\nYou could put it for every 60 seconds.\nif you want to basically change what you're getting out here.\nNow here's the thing that I've found to work really well.\nIf I wanted to, I could actually just change the prompt and ask\nquestions of the audio directly.\nFor podcasts, once you've got this transcript though, it doesn't really\nmake a lot of sense to keep using the audio unless you, had something\nwhere perhaps they're playing music and they're talking about the music,\nand you wanna find out something about the music, that kind of thing.\nwhat I tend to do is basically take that, transcript and now I prompt that.\nand this is a simple prompt that I've come up with most welcome to change it.\nI've got different variations, et cetera.\nHere it basically saying, make a set of notes in the form of bullet points with\nthe timestamp at the end of each idea, in the form of, hours, minutes, seconds.\nTo summarize this podcast, the bullets should be based on the idea\nand don't need to be sequential.\nSo if someone's talking about an idea at one point and then they come back to\nit later on, we kind of want like all those, collected together, structure\nthe ideas with headings and subheadings.\nDon't include any prefix.\nHere is the transcript below.\nAnd then I pass in that transcript that we've just, processed\nto basically get it down.\nSo you can see here we've got this prompt that we've just put\nin, plus the process transcript.\nNow I get out a whole summary with a bunch of ideas, in here, and\nyou can see that each of them, has got the timestamp at the end.\nSo if I do wanna go to, one of the particular things, and check\nit out, I can actually, go to it.\nNow it may not be always a hundred percent accurate with these, but I find\nthat it is actually pretty good with the Gemini 2.5 Pro it's much better than the\nold 1.5, used to be this kind of thing.\nSo you can actually get it to basically give you those timestamps.\nand then if you wanted to, you could actually code up a UI\nthat could just auto go to those, timestamps in the recording.\nSo if you did want to go and check something like here, we wanna see,\nokay, these five roles in any system for wealth creation, we can actually\njust go to that timestamp, and then start going through and looking at it.\nWe can see that each one has got a different timestamp in there.\nand sure enough, we've got each of these five things laid out, et cetera.\nAnd then if we just come in here, we can actually play the audio file\nand go through it and look at this.\nNow, of course, we could put in, saving this to, a text file if\nwe want to use it, posting it somewhere, that kind of thing as well.\nnow one of the other things I mentioned early on is that if the podcast is\nmore than two hours, the challenge is you're not gonna have enough\ntokens to generate all of it out.\nSo what you can do is you can actually just put it in your prompt.\nI want you to start the transcript at one hour, 30 minutes into this podcast.\nand then get the transcript out for that.\nAnd then you can use a little bit of fuzzy matching to actually join it.\nSo normally what I would do for that kind of thing is do some kind of overlap\nwhere I've got a few minutes overlap of\nthe first segment is gonna end maybe at one hour, 32.\nthe second segment is gonna start at one hour 30.\nAnd then I know on both of those transcripts, I should have two minutes\nthat kind of overlap, which I can then get it to work out what to cut or something.\nNow that one you can sort of experiment.\nAgain, the Gemini 2.5 pro model, is just so much better at this\nnow than the old models were.\nAlright, so just to finish up, if you've got any sort of transcription needs,\nof course, using raw transcription models is still, a useful thing.\nThough things like whisper, et cetera, out of the box don't actually\ngive you that diarization, in the way that this kind of can do.\nAlso with AI Studio at the moment, you can actually just make free calls like this.\nthis whole notebook was just two calls.\nSo you can set this up to just go and transcribe the podcast that you're\ninterested in, the audio interviews that you're interested in, And put\nthis all together so that you've got, a set of notes, et cetera.\nand you can even prompt it to do things like NotebookLM, where it\nwould turn it into a podcast and then you can send that to a TTS system to\nmake a podcast about a podcast or an abbreviated podcast about a podcast.\nI think there are lots of fun things that we'll see happening in the future\nwith, now the TTS systems, being able to copy people's voices, That you may\nbe even able to just take one of the voices off the original recording, have\nit do the summary for you, at the end.\nNow, of course you have to be careful of legal issues, et cetera.\nthis really is a cool way to be able to take audio content and just reduce it to a\nform that you can consume it much quicker, Anyway, in an upcoming video, I'll also\nlook at doing the same for YouTube videos.\nAnd some of the techniques that you can use this model for\ndoing YouTube videos as well."
  },
  {
    "role": "assistant",
    "content": "Is goed. Wat wil je erover vragen?"
  }
]